# Awesome Machine Learning ü§ñ

[![github](https://img.shields.io/badge/GitHub-Repository-blue.svg)](https://github.com/ChenyuanLiu92/awesome-machine-learning)
[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/ChenyuanLiu92/awesome-machine-learning/issues)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A curated list of cutting-edge machine learning papers, focusing on the latest research, open-source project, and trends in artificial intelligence.

## üìñ Table of Contents

- [Latest Papers](#-latest-papers)
- [Blogs & News](#-news)
- [Open Source Projects](#-open-source-projects)
- [Research Trends](#-research-trends)
- [How to Contribute](#-how-to-contribute)

## üìö Latest Papers

### üåç Large Multi-Modal Models (LMMs)

#### Vision Language Models (VLA)
- **WorldVLA: Towards Autoregressive Action World Model** *(June 2025) &#160;&#160;[[Paper]](https://arxiv.org/abs/2506.21539) &#160; [[Code]](https://github.com/alibaba-damo-academy/WorldVLA)

- **BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models** *(June 2025) &#160;&#160;[[Paper]](https://arxiv.org/abs/2506.07961) &#160; [[Code]](https://github.com/BridgeVLA/BridgeVLA)

- **UniVLA: Learning to Act Anywhere with Task-centric Latent Actions** *(May 2025) &#160;&#160;[[Paper]](https://arxiv.org/abs/2505.06111) &#160; [[Code]](https://github.com/OpenDriveLab/UniVLA)

- **OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning**  *(May 2025) &#160;&#160;[[Paper]](https://arxiv.org/abs/2505.11917) &#160;&#160; [[Code]](https://github.com/Fanqi-Lin/OneTwoVLA)

- **CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models**  *(March 2025) &#160;&#160;[[Paper]](https://arxiv.org/abs/2503.22020) &#160;&#160; [[Code]](https://cot-vla.github.io/)

- **MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation**  *(March 2025) &#160;&#160;[[Paper]](https://arxiv.org/abs/2503.20384) &#160;&#160; [[Code]](https://github.com/RoyZry98/MoLe-VLA-Pytorch/)



- **RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation** *(October 2024)  &#160;&#160; [[Paper]](https://arxiv.org/abs/2410.07864) &#160; [[Code]](https://github.com/thu-ml/RoboticsDiffusionTransformer)

- **œÄ0: A Vision-Language-Action Flow Model for General Robot Control**  &#160;&#160; [[Paper]](https://www.physicalintelligence.company/download/pi0.pdf) &#160;  [[Code]](https://github.com/Physical-Intelligence/openpi?tab=readme-ov-file)



### üí¨ Large Language Models
- Coming soon...

### üé® Generative AI
#### Video Generation
- **Make-A-Video: Text-to-Video Generation without Text-Video Data**   *(September 2022) &#160;&#160; [[Paper]](https://arxiv.org/abs/2209.14792) &#160;  [[Code]](https://github.com/lucidrains/make-a-video-pytorch)


### üëÄ Computer Vision

#### Object Detection
- **Detect Anything 3D in the Wild** *(April 2025) &#160;&#160;[[Paper]](https://arxiv.org/abs/2504.07958) &#160; [[Code]](https://github.com/OpenDriveLab/DetAny3D)

#### Visual Segmentation
- **SAM 2: Segment Anything in Images and Videos** *(August 2024) &#160;&#160;[[Paper]](https://arxiv.org/abs/2408.00714) &#160; [[Code]](https://github.com/facebookresearch/sam2)


- **Segment Anything**  *(April 2023) &#160;&#160;[[Paper]](https://arxiv.org/abs/2304.02643) &#160;&#160; [[Code]](https://github.com/facebookresearch/segment-anything)

### üîç Natural Language Processing
- Coming soon...

### üß¨ AI for Science
- Coming soon...

### üì∞ Monthly Paper Highlights

#### July 2025
#### June 2024
#### May 2024
## üì∞ News

### Talks

- **Towards End-to-End Generative Modeling** *(June 2025) &#160;&#160; [[Video]](https://www.youtube.com/watch?v=r-fgrZ0Ve74)
&#160; CVPR 2025  &#160; Kaiming He 
<!-- - **Title** 
  - [Source Link](https://github.com/BridgeVLA/BridgeVLA)
  - Description -->

## üöÄ Open Source Projects

### Model & Algorithm Implementations
<!-- - **Transformers** - Hugging Face's pre-trained model library
- **Detectron2** - Facebook's object detection platform
- **OpenMMLab** - Multimedia laboratory's open-source algorithm library

### Tools & Platforms
- **Weights & Biases** - Experiment tracking and visualization
- **Neptune** - Machine learning experiment management
- **DVC** - Data version control -->

## üìà Research Trends

### 2025 Hot Research Topics
<!-- 1. **Multimodal Foundation Models** - Unified models processing text, image, audio, and video
2. **Efficient Model Architectures** - Alternatives to Transformers (Mamba, RetNet)
3. **AI Alignment & Safety** - Constitutional AI, RLHF, and safety research
4. **Agentic AI Systems** - Autonomous agents capable of complex reasoning and action
5. **AI for Scientific Discovery** - Applications in biology, chemistry, and physics -->

### Emerging Research Areas
<!-- - **Mechanistic Interpretability** - Understanding how neural networks work internally
- **Scaling Laws** - Predicting model performance from compute and data
- **Few-Shot Learning** - Learning from minimal examples
- **Continual Learning** - Learning without forgetting previous knowledge
- **Federated Learning** - Training models across distributed data sources -->

## üìÖ Todo

- [ ‚úÖ ] Advance in LLMs Area
- [ ‚úÖ ] Advance in Computer Vision
- [ ‚úÖ ] Advance in Generative AI

## ü§ù How to Contribute

We welcome any form of contribution! Please follow these steps:

1. **Fork** this repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a **Pull Request**

### üõ†Ô∏è Using Automation Tools

This repository includes helpful automation scripts in the [`scripts/`](scripts/) directory:

- **`scripts/update_and_sort_papers.sh`** - Automatically extract dates from arXiv papers and sort by publication date
- **`scripts/arxiv_date_extractor.sh`** - Extract metadata from individual arXiv papers
- **`scripts/install_hook.sh`** - Set up git hook for full automation (dates + "Last Updated")

**Quick workflow for adding papers:**
```bash
# 1. Set up full automation (one-time, run from anywhere)
./scripts/install_hook.sh

# 2. Add papers to README.md manually, following this format:
- **Paper Title** &#160;&#160;[[Paper]](Paper Link) &#160;&#160; [[Code]](Code Link)

# 3. Commit and push - FULL automation!
git add . && git commit -m "Add new papers" && git push
# Pre-commit: Extracts dates and sorts papers automatically
# Pre-push: Updates "Last Updated" date automatically
```

See [`scripts/README.md`](scripts/README.md) for detailed usage instructions.

### Contribution Guidelines
- Ensure links are valid and relevant
- Provide brief but informative descriptions
- Organize content according to existing format
- Prioritize quality over quantity
- Use the automation scripts to maintain consistent formatting

## üìû Contact

If you have any questions or suggestions, please reach out through:

- Create an [Issue](https://github.com/ChenyuanLiu92/awesome-machine-learning/issues)
- Send email to: liuchenyuan23@mails.ucas.edu.cn
<!-- - Discuss in [Discussions](https://github.com/ChenyuanLiu92/awesome-machine-learning/discussions) -->

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ‚≠ê Star History

If you find this project helpful, please give us a ‚≠êÔ∏è!

[![Star History Chart](https://api.star-history.com/svg?repos=ChenyuanLiu92e/awesome-machine-learning&type=Date)](https://star-history.com/#ChenyuanLiu92/awesome-machine-learning&Date)

---

**Last Updated**: July 07, 2025

**Maintainer**: [David Liu](https://github.com/ChenyuanLiu92)

*Continuously updated, contributions and follows are welcome!*
